{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n",
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "from src.train import train_epoch, evaluate\n",
    "import torch\n",
    "from src.train.config import *\n",
    "from src.preprocessing.config import *\n",
    "from src.preprocessing import vocab_transform\n",
    "from src.models.transformer import Seq2SeqTransformer\n",
    "import torch.nn as nn\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb#ch0000004?line=4'>5</a>\u001b[0m     start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb#ch0000004?line=5'>6</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(transformer, optimizer, loss_fn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb#ch0000004?line=6'>7</a>\u001b[0m     end_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/iref/PycharmProjects/SPBU_NMT/notebooks/training_showcase.ipynb#ch0000004?line=7'>8</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(transformer)\n",
      "File \u001b[0;32m~/PycharmProjects/SPBU_NMT/src/train/train.py:24\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/train/train.py?line=19'>20</a>\u001b[0m tgt_input \u001b[39m=\u001b[39m tgt[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/train/train.py?line=21'>22</a>\u001b[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001b[39m=\u001b[39m create_mask(src, tgt_input)\n\u001b[0;32m---> <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/train/train.py?line=23'>24</a>\u001b[0m logits \u001b[39m=\u001b[39m model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/train/train.py?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/train/train.py?line=27'>28</a>\u001b[0m tgt_out \u001b[39m=\u001b[39m tgt[\u001b[39m1\u001b[39m:, :]\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/PycharmProjects/SPBU_NMT/src/models/transformer.py:70\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[0;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/models/transformer.py?line=67'>68</a>\u001b[0m src_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_tok_emb(src))\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/models/transformer.py?line=68'>69</a>\u001b[0m tgt_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_tok_emb(trg))\n\u001b[0;32m---> <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/models/transformer.py?line=69'>70</a>\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(src_emb, tgt_emb, src_mask, tgt_mask, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/models/transformer.py?line=70'>71</a>\u001b[0m                         src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[1;32m     <a href='file:///home/iref/PycharmProjects/SPBU_NMT/src/models/transformer.py?line=71'>72</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(outs)\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py:5341\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py?line=5338'>5339</a>\u001b[0m     attn_mask \u001b[39m=\u001b[39m key_padding_mask\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py?line=5339'>5340</a>\u001b[0m \u001b[39melif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mbool:\n\u001b[0;32m-> <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py?line=5340'>5341</a>\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39;49mlogical_or(key_padding_mask)\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py?line=5341'>5342</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/nn/functional.py?line=5342'>5343</a>\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mmasked_fill(key_padding_mask, \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    if epoch % 3 == 0:\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()},\n",
    "        '../data/interim/transf_cp.tar',\n",
    "        )\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4766376c3a24cbdf01164fe688b3f4b45072453258767122389f685454cf3e02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spbu_nmt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
