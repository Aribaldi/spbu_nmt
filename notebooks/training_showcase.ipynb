{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n",
      "/home/iref/.conda/envs/spbu_nmt/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "from src.train import train_epoch, evaluate\n",
    "import torch\n",
    "from src.train.config import *\n",
    "from src.preprocessing.config import *\n",
    "from src.preprocessing import vocab_transform\n",
    "from src.models.transformer import Seq2SeqTransformer\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timeit import default_timer as timer\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "INITIAL_TRAIN = False\n",
    "MODELS_PATH = '../data/interim/transf_cp.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19214, 10837)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE, TGT_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "if INITIAL_TRAIN:\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(MODELS_PATH, map_location=DEVICE)\n",
    "    transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wrapper(epoch_num, model, optimizer, loss_fn, run_name):\n",
    "    global_val_loss = 1e10\n",
    "    writer = SummaryWriter(f'../data/interim/runs/{run_name}')\n",
    "    for epoch in range(epoch_num + 1, epoch_num+11):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(model, optimizer, loss_fn)\n",
    "        end_time = timer()\n",
    "        val_loss = evaluate(model, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "        writer.add_scalars('Training vs validation loss', {'Training': train_loss, 'Validation': val_loss}, epoch)\n",
    "        if val_loss < global_val_loss:\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': model.state_dict()},\n",
    "            f'../data/interim/runs/{run_name}/transf_cp.tar',\n",
    "            )\n",
    "            print('## Vall loss decreased, model succefully saved ##')\n",
    "            global_val_loss = val_loss\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Vall loss decreased, model succefully saved ##\n",
      "Epoch: 10, Train loss: 2.155, Val loss: 3.690, Epoch time = 102.823s\n",
      "Epoch: 11, Train loss: 1.966, Val loss: 3.755, Epoch time = 100.174s\n",
      "Epoch: 12, Train loss: 1.789, Val loss: 3.808, Epoch time = 101.949s\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 13, Train loss: 1.632, Val loss: 3.826, Epoch time = 102.310s\n",
      "Epoch: 14, Train loss: 1.430, Val loss: 3.725, Epoch time = 101.608s\n",
      "Epoch: 15, Train loss: 1.371, Val loss: 3.725, Epoch time = 105.622s\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 16, Train loss: 1.335, Val loss: 3.729, Epoch time = 105.825s\n",
      "Epoch: 17, Train loss: 1.314, Val loss: 3.720, Epoch time = 102.386s\n",
      "Epoch: 18, Train loss: 1.303, Val loss: 3.720, Epoch time = 101.134s\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 19, Train loss: 1.299, Val loss: 3.718, Epoch time = 101.474s\n"
     ]
    }
   ],
   "source": [
    "train_wrapper(epoch, transformer, optimizer, loss_fn, 'try_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4766376c3a24cbdf01164fe688b3f4b45072453258767122389f685454cf3e02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spbu_nmt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
